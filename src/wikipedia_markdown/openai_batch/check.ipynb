{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-26T17:49:20.095444Z",
     "start_time": "2025-01-26T17:49:19.807289Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai_token = getenv(\"OPENAI_TOKEN\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "batch = client.batches.retrieve(\"batch_67967539c8dc81908035d1d8ff4a3341\")\n",
    "print(batch)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_67967539c8dc81908035d1d8ff4a3341', completion_window='24h', created_at=1737913657, endpoint='/v1/chat/completions', input_file_id='file-1W1Kep56X6eoFPSiwTWisF', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-3HsOxKAGZ0zQ6yWnKXPSWwU8. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1738000057, failed_at=1737913658, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_input_109.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
