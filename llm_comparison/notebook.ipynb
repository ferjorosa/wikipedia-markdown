{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Objective\n",
    "\n",
    "This Jupyter notebook aims to explore the transformation of text from SimpleWikipedia using markdown. We are going to compare several models, both for short and long texts.\n",
    "\n",
    "**For Short Texts (Token Count <= 3,000)**\n",
    "- **Direct Transformation**: If the text contains fewer than 7,000 tokens, it will be transformed directly without segmentation. This approach is straightforward and efficient for shorter texts.\n",
    "\n",
    "**For Long Texts (Token Count > 3,000)**\n",
    "- **Segmented Transformation**: For longer texts exceeding 7,000 tokens, the text will be divided into manageable sections. Each section will be transformed individually, and the results will be combined to form the final output.\n",
    "\n",
    "#### Rationale\n",
    "\n",
    "1. **Model Limitations**: Many models lack the capacity to handle long contexts effectively while maintaining high quality. Segmenting the text helps mitigate this limitation.\n",
    "2. **Information Preservation**: We observed that models often truncate or significantly shorten formatted texts (e.g., reducing 11,000 tokens to 2,300 tokens), leading to substantial information loss. By processing sections individually, we aim to preserve the integrity and completeness of the original content.\n",
    "\n",
    "#### Note on 3,000 Token Limit\n",
    "\n",
    "The maximum output depends on the model. For example Llama models have a maximum of 4k-8k of tokens, GPT-4o of 16k, Deepseek of 8k, etc.\n",
    "\n",
    "The 3,000-token limit is chosen to provide a buffer, ensuring that even if the token count increases after transforming the text into markdown, the output remains within the model's capacity.\n",
    "\n",
    "#### Models\n",
    "\n",
    "We selected several cost-effective models\n",
    "\n",
    "* `meta-llama/llama-3.2-1b-instruct`\n",
    "    * \\$0.01/M input tokens\n",
    "    * \\$0.01/M output tokens\n",
    "* `meta-llama/llama-3.2-3b-instruct`\n",
    "    * \\$0.015/M input tokens\n",
    "    * \\$0.025/M output tokens\n",
    "* `nousresearch/hermes-2-pro-llama-3-8b`\n",
    "    * \\$0.025/M input tokens\n",
    "    * \\$0.04/M output tokens\n",
    "* `mistralai/ministral-8b`\n",
    "    * \\$0.1/M input tokens\n",
    "    * \\$0.1/M output tokens\n",
    "* `mistralai/mistral-nemo-12b`\n",
    "    * \\$0.035/M input tokens\n",
    "    * \\$0.08/M output tokens\n",
    "* `nousresearch/hermes-3-llama-3.1-70b`\n",
    "    * \\$0.12/M input tokens\n",
    "    * \\$0.3/M output tokens\n",
    "* `openai/gpt-4o-mini`\n",
    "    * \\$0.15/M input tokens\n",
    "    * \\$0.6/M output tokens\n",
    "* `deepseek/deepseek-chat`\n",
    "    * \\$0.14/M input tokens\n",
    "    * \\$0.28/M output tokens"
   ],
   "id": "b1782e66c381c64d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "from os import getenv\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils.tokenizer import count_tokens\n",
    "from src.format_articles import (\n",
    "    convert_text_to_markdown,\n",
    "    convert_long_text_to_markdown\n",
    ")\n",
    "\n",
    "def _load_yaml(path):\n",
    "    with open(path) as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "load_dotenv()\n",
    "tqdm.pandas()\n",
    "huggingface_token = getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "base_path = Path(\"..\")"
   ],
   "id": "e5a1b14472c7d7be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1 - Load data",
   "id": "a7263b6be4c8631b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "data = pd.read_parquet(base_path / \"data/parsed/articles.parquet\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2 - Count tokens\n",
    "\n",
    "We should use the specific tokenizer for each case, but this is just to get an estimation..."
   ],
   "id": "2ddf931413a957b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_hf = \"deepseek-ai/DeepSeek-V3\"\n",
    "\n",
    "# Load the tokenizer using the Hugging Face token for authentication\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_hf, token=huggingface_token)\n",
    "\n",
    "# Apply the function to each row in the 'text' column with a progress bar\n",
    "data['token_count'] = data['text'].progress_apply(lambda text: count_tokens(tokenizer, text))\n",
    "\n",
    "# Print the first row to verify\n",
    "print(f\"Token count: {data['token_count'].sum()}\")"
   ],
   "id": "24073b7f63faad27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Short article",
   "id": "bc44aae42bd5c67d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "short_article = data[data[\"id\"] == 13].iloc[0]\n",
    "\n",
    "short_article_directory = Path(\"./short\")\n",
    "short_article_path = short_article_directory / \"base.md\"\n",
    "short_article_directory.mkdir(parents=True, exist_ok=True)\n",
    "short_article_path.write_text(short_article[\"text\"], encoding=\"utf-8\")\n",
    "\n",
    "print(short_article[\"text\"])\n",
    "print(\"\\n================\\n\")\n",
    "print(short_article[\"token_count\"])"
   ],
   "id": "d94d93ae84af7de4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Long article",
   "id": "87d4a9ea7384bea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "long_article = data[data[\"id\"] == 3077].iloc[0]\n",
    "\n",
    "long_article_directory = Path(\"./long\")\n",
    "long_article_path = long_article_directory / \"base.md\"\n",
    "long_article_directory.mkdir(parents=True, exist_ok=True)\n",
    "long_article_path.write_text(long_article[\"text\"], encoding=\"utf-8\")\n",
    "\n",
    "print(long_article[\"text\"])\n",
    "print(\"\\n================\\n\")\n",
    "print(long_article[\"token_count\"])"
   ],
   "id": "1130531dc6a67c04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3 - Compare models",
   "id": "a9cc56e06b4e1bf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load prompt\n",
    "prompts_path = base_path / \"prompts.yaml\"\n",
    "with open(prompts_path, \"r\") as file:\n",
    "    prompts = yaml.safe_load(file)"
   ],
   "id": "f5f395ab6711aeee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.1 - Llama-3.2 1B",
   "id": "8887c57863f0e92e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "short_article_formatted_llama_1b = convert_text_to_markdown(\n",
    "    model_openrouter=\"meta-llama/llama-3.2-1b-instruct\",\n",
    "    raw_text=short_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "short_article_formatted_llama_1b_path = short_article_directory / \"llama_1b.md\"\n",
    "short_article_formatted_llama_1b_path.write_text(short_article_formatted_llama_1b, encoding=\"utf-8\")"
   ],
   "id": "ae887252b79f221c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llama_1b_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    token=huggingface_token\n",
    ")\n",
    "\n",
    "long_article_formatted_llama_1b = convert_long_text_to_markdown(\n",
    "    model_openrouter=\"meta-llama/llama-3.2-1b-instruct\",\n",
    "    raw_text=long_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    tokenizer=llama_1b_tokenizer,\n",
    "    max_tokens=3000,\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "\n",
    "long_article_formatted_llama_1b_path = long_article_directory / \"llama_1b.md\"\n",
    "long_article_formatted_llama_1b_path.write_text(long_article_formatted_llama_1b, encoding=\"utf-8\")"
   ],
   "id": "58028d4c6b65a2db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.2 - Llama-3.2 3B",
   "id": "156161870e4caace"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "short_article_formatted_llama_3b = convert_text_to_markdown(\n",
    "    model_openrouter=\"meta-llama/llama-3.2-3b-instruct\",\n",
    "    raw_text=short_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "short_article_formatted_llama_3b_path = short_article_directory / \"llama_3b.md\"\n",
    "short_article_formatted_llama_3b_path.write_text(short_article_formatted_llama_3b, encoding=\"utf-8\")"
   ],
   "id": "b8e8551638f804e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llama_3b_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    token=huggingface_token\n",
    ")\n",
    "\n",
    "long_article_formatted_llama_3b = convert_long_text_to_markdown(\n",
    "    model_openrouter=\"meta-llama/llama-3.2-3b-instruct\",\n",
    "    raw_text=long_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    tokenizer=llama_3b_tokenizer,\n",
    "    max_tokens=3000,\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "\n",
    "long_article_formatted_llama_3b_path = long_article_directory / \"llama_3b.md\"\n",
    "long_article_formatted_llama_3b_path.write_text(long_article_formatted_llama_3b, encoding=\"utf-8\")"
   ],
   "id": "31532878951f4290",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.3 - Llama-3.1 8B",
   "id": "946c817b5a9ee4c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "short_article_formatted_llama_8b = convert_text_to_markdown(\n",
    "    model_openrouter=\"nousresearch/hermes-2-pro-llama-3-8b\",\n",
    "    raw_text=short_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "short_article_formatted_llama_8b_path = short_article_directory / \"llama_8b.md\"\n",
    "short_article_formatted_llama_8b_path.write_text(short_article_formatted_llama_8b, encoding=\"utf-8\")"
   ],
   "id": "a04afc431d82fedd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llama_8b_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"NousResearch/Hermes-2-Pro-Llama-3-8B\",\n",
    "    token=huggingface_token\n",
    ")\n",
    "\n",
    "long_article_formatted_llama_8b = convert_long_text_to_markdown(\n",
    "    model_openrouter=\"nousresearch/hermes-2-pro-llama-3-8b\",\n",
    "    raw_text=long_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    tokenizer=llama_8b_tokenizer,\n",
    "    max_tokens=3000,\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "\n",
    "long_article_formatted_llama_8b_path = long_article_directory / \"llama_8b.md\"\n",
    "long_article_formatted_llama_8b_path.write_text(long_article_formatted_llama_3b, encoding=\"utf-8\")"
   ],
   "id": "549bfcc69ab51324",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.4 - Ministral 8B",
   "id": "40418912ecf89f16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "short_article_formatted_ministral_8b = convert_text_to_markdown(\n",
    "    model_openrouter=\"mistralai/ministral-8b\",\n",
    "    raw_text=short_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "short_article_formatted_ministral_8b_path = short_article_directory / \"ministral_8b.md\"\n",
    "short_article_formatted_ministral_8b_path.write_text(short_article_formatted_ministral_8b, encoding=\"utf-8\")"
   ],
   "id": "e6072f5bc25a42b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ministral_8b_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Ministral-8B-Instruct-2410\",\n",
    "    token=huggingface_token\n",
    ")\n",
    "\n",
    "long_article_formatted_ministral_8b = convert_long_text_to_markdown(\n",
    "    model_openrouter=\"mistralai/ministral-8b\",\n",
    "    raw_text=long_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    tokenizer=ministral_8b_tokenizer,\n",
    "    max_tokens=3000,\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "\n",
    "long_article_formatted_ministral_8b_path = long_article_directory / \"ministral_8b.md\"\n",
    "long_article_formatted_ministral_8b_path.write_text(long_article_formatted_ministral_8b, encoding=\"utf-8\")"
   ],
   "id": "2a710f3190cb19fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.5 - Mistral Nemo 12B",
   "id": "3729cfd8c7c3afea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "short_article_formatted_nemo_12b = convert_text_to_markdown(\n",
    "    model_openrouter=\"mistralai/mistral-nemo\",\n",
    "    raw_text=short_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "short_article_formatted_nemo_12b_path = short_article_directory / \"nemo_12b.md\"\n",
    "short_article_formatted_nemo_12b_path.write_text(short_article_formatted_nemo_12b, encoding=\"utf-8\")"
   ],
   "id": "d4861a28d0a9d4c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.6 - Llama-3.1 70B",
   "id": "b27b474a90ccb816"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nemo_12b_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    token=huggingface_token\n",
    ")\n",
    "\n",
    "long_article_formatted_nemo_12b = convert_long_text_to_markdown(\n",
    "    model_openrouter=\"mistralai/mistral-nemo\",\n",
    "    raw_text=long_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    tokenizer=nemo_12b_tokenizer,\n",
    "    max_tokens=3000,\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "\n",
    "long_article_formatted_nemo_12b_path = long_article_directory / \"nemo_12b.md\"\n",
    "long_article_formatted_nemo_12b_path.write_text(long_article_formatted_nemo_12b, encoding=\"utf-8\")"
   ],
   "id": "690cd24b810b9af7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.7 - GPT-4o-mini",
   "id": "a44f75a739689f0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "short_article_formatted_gpt4o_mini = convert_text_to_markdown(\n",
    "    model_openrouter=\"openai/gpt-4o-mini\",\n",
    "    raw_text=short_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "short_article_formatted_gpt4o_mini_path = short_article_directory / \"gpt4o_mini.md\"\n",
    "short_article_formatted_gpt4o_mini_path.write_text(short_article_formatted_gpt4o_mini, encoding=\"utf-8\")"
   ],
   "id": "f1c62b6c2bb19b70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gpt4o_mini_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Xenova/gpt-4o\",\n",
    "    token=huggingface_token\n",
    ")\n",
    "\n",
    "long_article_formatted_gpt4o_mini = convert_long_text_to_markdown(\n",
    "    model_openrouter=\"openai/gpt-4o-mini\",\n",
    "    raw_text=long_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    tokenizer=gpt4o_mini_tokenizer,\n",
    "    max_tokens=3000,\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "\n",
    "long_article_formatted_gpt4o_mini_path = long_article_directory / \"gpt4o_mini.md\"\n",
    "long_article_formatted_gpt4o_mini_path.write_text(long_article_formatted_gpt4o_mini, encoding=\"utf-8\")"
   ],
   "id": "f97e8c5c056b1738",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.8 - DeepSeek V3",
   "id": "130825febfbfb909"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "short_article_formatted_deepseek_v3 = convert_text_to_markdown(\n",
    "    model_openrouter=\"deepseek/deepseek-chat\",\n",
    "    raw_text=short_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "short_article_formatted_deepseek_v3_path = short_article_directory / \"deepseek_v3.md\"\n",
    "short_article_formatted_deepseek_v3_path.write_text(short_article_formatted_deepseek_v3, encoding=\"utf-8\")"
   ],
   "id": "939c98891c56f11a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "deepseek_v3_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-V3\",\n",
    "    token=huggingface_token\n",
    ")\n",
    "\n",
    "long_article_formatted_deepseek_v3 = convert_long_text_to_markdown(\n",
    "    model_openrouter=\"deepseek/deepseek-chat\",\n",
    "    raw_text=long_article[\"text\"],\n",
    "    template=prompts[\"format_markdown\"],\n",
    "    tokenizer=deepseek_v3_tokenizer,\n",
    "    max_tokens=3000,\n",
    "    apply_simple_formatting=True,\n",
    "    apply_llm_formatting=True,\n",
    ")\n",
    "\n",
    "long_article_formatted_deepseek_v3_path = long_article_directory / \"deepseek_v3.md\"\n",
    "long_article_formatted_deepseek_v3_path.write_text(long_article_formatted_deepseek_v3, encoding=\"utf-8\")"
   ],
   "id": "e055948d15d965db",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
